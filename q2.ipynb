{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class shallow_nn:\n",
    "    \"\"\" a shallow neural network\n",
    "    dim\n",
    "        input = 2\n",
    "        hidden layer = n\n",
    "        output = 1\n",
    "    \"\"\"    \n",
    "\n",
    "    def __init__(self, n, param_hidden, param_output, learning_rate, ttl_layer=None) -> None:\n",
    "        print(\"enter __init__\")\n",
    "        # fool-proof\n",
    "        self.training = False\n",
    "        \n",
    "        # number of node\n",
    "        self.num_input = 2\n",
    "        self.num_hidden = n\n",
    "        self.num_output = 1\n",
    "        \n",
    "        # activation function\n",
    "        self.param_hidden = param_hidden\n",
    "        self.param_output = param_output\n",
    "        \n",
    "        # hyper param\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # init layer\n",
    "        if ttl_layer == None:\n",
    "            np.random.seed(0) # for reproducibility\n",
    "            self.weights_input_hidden = np.random.uniform(size=(self.num_input, self.num_hidden))\n",
    "            # print(f\"\\tself.weights_input_hidden.shape = {self.weights_input_hidden.shape}\")\n",
    "            self.bias_hidden = np.random.uniform(size=(1, self.num_hidden))\n",
    "            self.weights_hidden_output = np.random.uniform(size=(self.num_hidden, self.num_output))\n",
    "            self.bias_output = np.random.uniform(size=(1, self.num_output))\n",
    "        else:\n",
    "            self.weights_input_hidden = ttl_layer[0]\n",
    "            self.bias_hidden = ttl_layer[1]\n",
    "            self.weights_hidden_output = ttl_layer[2]\n",
    "            self.bias_output = ttl_layer[3]\n",
    "\n",
    "            \n",
    "        print(f\"\\tself.weights_input_hidden.shape = {self.weights_input_hidden.shape}\")\n",
    "        print(f\"\\tself.bias_hidden.shape = {self.bias_hidden.shape}\")\n",
    "        print(f\"\\tself.weights_hidden_output.shape = {self.weights_hidden_output.shape}\")\n",
    "        print(f\"\\tself.bias_output.shape = {self.bias_output.shape}\")\n",
    "        print(\"exit __init__\")\n",
    "  \n",
    "    def get_layer(self):\n",
    "        ttl_layer = []\n",
    "        ttl_layer.append(self.weights_input_hidden)\n",
    "        ttl_layer.append(self.bias_hidden)\n",
    "        ttl_layer.append(self.weights_hidden_output)\n",
    "        ttl_layer.append(self.bias_output)\n",
    "        return ttl_layer\n",
    "  \n",
    "    def act_hidden(self,x):\n",
    "        \"\"\" tanh\n",
    "        \"\"\"\n",
    "        print(\"enter act_hidden\")\n",
    "        # print(f\"\\tx = {x}\")\n",
    "        exponient = np.e**(2*self.param_hidden*x)\n",
    "        # print(f\"\\texponient = {exponient}\")\n",
    "        y = (exponient-1)/(exponient+1)\n",
    "        # print(f\"\\ty = {y}\")\n",
    "        print(\"exit act_hidden\")\n",
    "        return y\n",
    "    \n",
    "    def div_act_hidden(self,x):\n",
    "        \"\"\" tanh\n",
    "        \"\"\"\n",
    "        print(\"enter div_act_hidden\")\n",
    "        print(f\"\\tx = {x}\")\n",
    "        exponient = np.e**(self.param_hidden*x)\n",
    "        print(f\"\\texponient = {exponient}\")\n",
    "        exponient_minus = np.e**(-self.param_hidden*x)\n",
    "        print(f\"\\texponient_minus = {exponient_minus}\")\n",
    "        y = self.param_hidden*4/(exponient+exponient_minus)\n",
    "        print(f\"\\ty = {y}\")\n",
    "        print(\"exit div_act_hidden\")\n",
    "        return y\n",
    "    \n",
    "    def act_output(self, x):\n",
    "        \"\"\" linear\n",
    "        \"\"\"\n",
    "        y = self.param_hidden*x\n",
    "        return y\n",
    "    \n",
    "    def div_act_output(self, x):\n",
    "        \"\"\" linear\n",
    "        \"\"\"\n",
    "        y = self.param_hidden\n",
    "        return np.array([y])\n",
    "    \n",
    "    # def loss_function(self, pred, target):\n",
    "    #     \"\"\" compute the error\n",
    "    #     \"\"\"\n",
    "    #     error = target - pred\n",
    "    #     loss = np.mean(np.square(error))\n",
    "    #     return loss\n",
    "    \n",
    "    def forward(self, X, training=False):\n",
    "        \"\"\"\n",
    "        Var:\n",
    "            training: bool\n",
    "                True    backward later\n",
    "                False   forward only\n",
    "        \"\"\"\n",
    "        print(f\"enter forward, training={training}\")\n",
    "        # print(f\"\\ttype(X) = {type(X)}\")\n",
    "        # print(f\"\\tX.shape = {X.shape}\")\n",
    "        self.training = training\n",
    "        if self.training is True:\n",
    "            print(\"if self.training is True\")\n",
    "            self.data = X\n",
    "        else:\n",
    "            print(\"if self.training is False\")\n",
    "            self.data = None\n",
    "        \n",
    "        print()\n",
    "        # print(f\"\\ttype(X) = {type(X)}\")\n",
    "        # print(f\"\\tX.shape = {X.shape}\")\n",
    "        # print(f\"\\tself.weights_input_hidden.shape = {self.weights_input_hidden.shape}\")\n",
    "        print(f\"X = {X}\")\n",
    "        print(f\"self.weights_input_hidden = {self.weights_input_hidden}\")\n",
    "        self.hidden_layer_in = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        print(f\"\\tself.hidden_layer_in = {self.hidden_layer_in}\")\n",
    "        self.hidden_layer_out = self.act_hidden(self.hidden_layer_in)\n",
    "        # print(f\"\\tself.hidden_layer_out = {self.hidden_layer_out}\")\n",
    "        \n",
    "        # print(f\"\\tself.hidden_layer_out.shape = {self.hidden_layer_out.shape}\")\n",
    "        # print(f\"\\tself.weights_hidden_output.shape = {self.weights_hidden_output.shape}\")\n",
    "        self.output_layer_in = np.dot(self.hidden_layer_out, self.weights_hidden_output) + self.bias_output\n",
    "        # print(f\"\\tself.output_layer_in.shape = {self.output_layer_in.shape}\")\n",
    "        self.output_layer_out = self.act_output(self.output_layer_in)\n",
    "        \n",
    "        \n",
    "        # print(f\"\\tself.weights_input_hidden.shape = {self.weights_input_hidden.shape}\")\n",
    "        # print(f\"\\tself.bias_hidden.shape = {self.bias_hidden.shape}\")\n",
    "        # print(f\"\\tself.weights_hidden_output.shape = {self.weights_hidden_output.shape}\")\n",
    "        # print(f\"\\tself.bias_output.shape = {self.bias_output.shape}\")\n",
    "        \n",
    "        pred = self.output_layer_out\n",
    "        \n",
    "        print(f\"exit forward, training={training}\")\n",
    "        return pred\n",
    "    \n",
    "    def backward(self, target):\n",
    "        print(\"enter backward\")\n",
    "        if self.training is False:\n",
    "            print(\"\\tif self.training is False\")\n",
    "        else:\n",
    "            print(\"\\tif self.training is True\")\n",
    "            # error = target - self.output_layer_activation\n",
    "            # loss = 0.5*(np.square(error))\n",
    "            \n",
    "            # div_loss_2_error = error\n",
    "            # div_error_2_output = -1\n",
    "            \n",
    "            # compute gradient\n",
    "            # print(f\"\\ttarget.shape = {target.shape}\")\n",
    "            # print(f\"\\tself.output_layer_out = {self.output_layer_out.shape}\")\n",
    "            # print()\n",
    "            error_output = target - self.output_layer_out\n",
    "            print(\"\\tafter error_output\")\n",
    "            # print(f\"\\terror_output.shape = {error_output.shape}\")\n",
    "            # print(f\"\\tself.output_layer_i.shape = {self.output_layer_in.shape}\")\n",
    "            # print(f\"\\tself.div_act_output(self.output_layer_in).shape = {self.div_act_output(self.output_layer_in).shape}\")\n",
    "            # print()\n",
    "            \n",
    "            back_output = error_output * (-1) * self.div_act_output(self.output_layer_in)\n",
    "            print(\"\\tafter back_output\")\n",
    "            # print(f\"\\ttype(back_output) = {type(back_output)}\")\n",
    "            # print(f\"\\tback_output.shape = {back_output.shape}\")\n",
    "            # print(f\"\\ttype(self.hidden_layer_out) = {type(self.hidden_layer_out)}\")\n",
    "            # print(f\"\\tself.hidden_layer_out.shape = {self.hidden_layer_out.shape}\")\n",
    "            # print()\n",
    "            \n",
    "            # grad_output = back_output.dot(self.hidden_layer_out.T)\n",
    "            grad_output = self.hidden_layer_out.T.dot(back_output)\n",
    "            print(\"\\tafter grad_output\")\n",
    "            # print(f\"\\ttype(back_output) = {type(back_output)}\")\n",
    "            # print(f\"\\tback_output.shape = {back_output.shape}\")\n",
    "            # print(f\"\\ttype(self.weights_hidden_output) = {type(self.weights_hidden_output)}\")\n",
    "            # print(f\"\\tself.weights_hidden_output.shape = {self.weights_hidden_output.shape}\")\n",
    "            # print(f\"\\tself.div_act_hidden(self.hidden_layer_in).shape = {self.div_act_hidden(self.hidden_layer_in).shape}\")\n",
    "            # print()\n",
    "\n",
    "            back_hidden = back_output.dot(self.weights_hidden_output.T) * self.div_act_hidden(self.hidden_layer_in)\n",
    "            print(\"\\tafter back_hidden\")\n",
    "            print()\n",
    "            # grad_hidden = back_hidden.dot(self.data.T)\n",
    "            grad_hidden = self.data.T.dot(back_hidden)\n",
    "            print(\"\\tafter grad_hidden\")\n",
    "            print()\n",
    "            \n",
    "            # update weight and bias\n",
    "            self.weights_hidden_output -= grad_output * self.learning_rate\n",
    "            self.bias_output -= np.sum(back_output, axis=0, keepdims=True) * self.learning_rate\n",
    "            self.weights_input_hidden -= grad_hidden * self.learning_rate\n",
    "            self.bias_hidden -= np.sum(back_hidden, axis=0, keepdims=True) * self.learning_rate\n",
    "\n",
    "            # train 過後就關起來\n",
    "            self.training = False\n",
    "        print(\"exit backward\")\n",
    "\n",
    "    def evaluate(self, X, target):\n",
    "        pred = self.forward(X)\n",
    "        loss = 0.5*np.mean(np.square(target-pred))\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        print(\"enter predict\")\n",
    "        # print(f\"\\ttype(X) = {type(X)}\")\n",
    "        pred = self.forward(X)\n",
    "        print(\"exit predict\")\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "class normalization:\n",
    "    \n",
    "    def __init__(self, data) -> None:\n",
    "        self.data = data\n",
    "        \n",
    "        max = np.max(data)\n",
    "        self.min = np.min(data)\n",
    "        \n",
    "        self.range_original = max - self.min\n",
    "        # range_desired = 1\n",
    "\n",
    "    \n",
    "    def normalize(self, data):\n",
    "        new_data = (data-self.min)/self.range_original\n",
    "        return new_data\n",
    "    \n",
    "    def unnormalize(self, X):\n",
    "        raw_data = X*self.range_original + self.min\n",
    "        return raw_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3732, 4264],\n",
       "       [5859, 8891],\n",
       "       [5373, 6874],\n",
       "       ...,\n",
       "       [7168, 9948],\n",
       "       [5007, 2158],\n",
       "       [9685, 7721]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[3732, 4264],\n",
       "       [5859, 8891],\n",
       "       [5373, 6874],\n",
       "       ...,\n",
       "       [7168, 9948],\n",
       "       [5007, 2158],\n",
       "       [9685, 7721]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# construct training data and testing data\n",
    "\n",
    "np.random.seed(0)\n",
    "X_size = (20000,2)\n",
    "\n",
    "X = np.random.randint(1000, 9999, size=X_size)\n",
    "Y = np.sum(X, axis=1)\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "idx_testing = rng.choice(20000, 4000, replace=False, shuffle=False)\n",
    "\n",
    "# testing data\n",
    "X_test = X[idx_testing]\n",
    "Y_test = Y[idx_testing]\n",
    "\n",
    "# training data\n",
    "mask = np.ones(Y.shape, dtype=bool)\n",
    "mask[idx_testing] = False\n",
    "\n",
    "X_train = X[mask]\n",
    "Y_train = Y[mask]\n",
    "\n",
    "display(X)\n",
    "display(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76914527, 0.35667445],\n",
       "       [0.34578193, 0.73468934],\n",
       "       [0.10681338, 0.74924975],\n",
       "       ...,\n",
       "       [0.33011004, 0.05312882],\n",
       "       [0.0662443 , 0.84661554],\n",
       "       [0.960987  , 0.49805491]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1.23696788],\n",
       "       [1.19161943],\n",
       "       [0.96721129],\n",
       "       ...,\n",
       "       [0.49438702],\n",
       "       [1.024008  ],\n",
       "       [1.57019006]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess\n",
    "\n",
    "# normalize\n",
    "normalization_test = normalization(X_test)\n",
    "X_test_n = normalization_test.normalize(X_test)\n",
    "Y_test_n = normalization_test.normalize(Y_test)\n",
    "Y_test_n = Y_test_n.reshape((-1,1))\n",
    "display(X_test_n)\n",
    "display(Y_test_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint(nn: shallow_nn):\n",
    "    print(\"enter seve_checkpoint\")\n",
    "    loss = nn.evaluate(X_test_n, Y_test_n)\n",
    "    param = nn.get_layer()\n",
    "    print(\"exit seve_checkpoint\")\n",
    "    return loss, param "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter __init__\n",
      "\tself.weights_input_hidden.shape = (2, 5)\n",
      "\tself.bias_hidden.shape = (1, 5)\n",
      "\tself.weights_hidden_output.shape = (5, 1)\n",
      "\tself.bias_output.shape = (1, 1)\n",
      "exit __init__\n",
      "enter seve_checkpoint\n",
      "enter forward, training=False\n",
      "if self.training is False\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[0.5488135  0.71518937 0.60276338 0.54488318 0.4236548 ]\n",
      " [0.64589411 0.43758721 0.891773   0.96366276 0.38344152]]\n",
      "\tself.hidden_layer_in = [[1.44421627 1.23505562 1.3497298  1.68840484 0.53365194]\n",
      " [1.45602635 1.09768514 1.43164536 1.82200015 0.49923863]\n",
      " [1.33428167 0.93314882 1.30058845 1.70582154 0.40358152]\n",
      " ...\n",
      " [1.00720948 0.7882346  0.81440165 1.15666631 0.23126056]\n",
      " [1.3749048  0.94674027 1.36296308 1.77754391 0.42372832]\n",
      " [1.64081841 1.43412506 1.59144425 1.92917926 0.66913774]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=False\n",
      "exit seve_checkpoint\n",
      "enter forward, training=True\n",
      "if self.training is True\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[0.5488135  0.71518937 0.60276338 0.54488318 0.4236548 ]\n",
      " [0.64589411 0.43758721 0.891773   0.96366276 0.38344152]]\n",
      "\tself.hidden_layer_in = [[1.44421627 1.23505562 1.3497298  1.68840484 0.53365194]\n",
      " [1.45602635 1.09768514 1.43164536 1.82200015 0.49923863]\n",
      " [1.33428167 0.93314882 1.30058845 1.70582154 0.40358152]\n",
      " ...\n",
      " [1.00720948 0.7882346  0.81440165 1.15666631 0.23126056]\n",
      " [1.3749048  0.94674027 1.36296308 1.77754391 0.42372832]\n",
      " [1.64081841 1.43412506 1.59144425 1.92917926 0.66913774]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=True\n",
      "enter backward\n",
      "\tif self.training is True\n",
      "\tafter error_output\n",
      "\tafter back_output\n",
      "\tafter grad_output\n",
      "enter div_act_hidden\n",
      "\tx = [[1.44421627 1.23505562 1.3497298  1.68840484 0.53365194]\n",
      " [1.45602635 1.09768514 1.43164536 1.82200015 0.49923863]\n",
      " [1.33428167 0.93314882 1.30058845 1.70582154 0.40358152]\n",
      " ...\n",
      " [1.00720948 0.7882346  0.81440165 1.15666631 0.23126056]\n",
      " [1.3749048  0.94674027 1.36296308 1.77754391 0.42372832]\n",
      " [1.64081841 1.43412506 1.59144425 1.92917926 0.66913774]]\n",
      "\texponient = [[4.238529   3.43856975 3.85638341 5.41084268 1.70514804]\n",
      " [4.2888831  2.99721983 4.18558032 6.18421547 1.64746646]\n",
      " [3.79726727 2.54250248 3.67145651 5.50590711 1.49717728]\n",
      " ...\n",
      " [2.73795003 2.19950998 2.2578243  3.17931674 1.26018755]\n",
      " [3.95470021 2.57729468 3.90775516 5.91531004 1.52764651]\n",
      " [5.15939029 4.19597217 4.91083627 6.88385805 1.95255299]]\n",
      "\texponient_minus = [[0.23593091 0.29081859 0.25931032 0.1848141  0.58645934]\n",
      " [0.23316094 0.33364253 0.2389155  0.161702   0.60699263]\n",
      " [0.26334728 0.39331328 0.27237147 0.18162311 0.66792357]\n",
      " ...\n",
      " [0.36523676 0.45464672 0.44290426 0.31453299 0.79353268]\n",
      " [0.25286367 0.38800375 0.2559014  0.16905285 0.6546017 ]\n",
      " [0.19382135 0.23832379 0.20363131 0.14526738 0.51214999]]\n",
      "\ty = [[0.89396264 1.07256194 0.97188962 0.71484013 1.7454997 ]\n",
      " [0.88455574 1.20089021 0.90405781 0.63032651 1.77426152]\n",
      " [0.98507257 1.36248332 1.01424302 0.70329297 1.84748899]\n",
      " ...\n",
      " [1.28899749 1.50707002 1.48108183 1.14486893 1.94768496]\n",
      " [0.95066887 1.34893674 0.96069403 0.65742298 1.83297206]\n",
      " [0.74721499 0.90205977 0.78209509 0.56906084 1.6229136 ]]\n",
      "exit div_act_hidden\n",
      "\tafter back_hidden\n",
      "\n",
      "\tafter grad_hidden\n",
      "\n",
      "exit backward\n",
      "enter seve_checkpoint\n",
      "enter forward, training=False\n",
      "if self.training is False\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[-2.08600244e+00 -4.05303936e-02 -2.64480785e+01 -1.79828788e+01\n",
      "  -5.04271247e+01]\n",
      " [-1.92295871e+00 -3.42664729e-01 -2.46768047e+01 -1.60750159e+01\n",
      "  -5.01876205e+01]]\n",
      "\tself.hidden_layer_in = [[  -7.47793116   -1.36752662  -90.19118048  -60.79776531 -166.242091  ]\n",
      " [  -7.32170123   -1.47989991  -88.32223323  -59.26106904 -163.86476294]\n",
      " [  -6.85121133   -1.47520375  -82.36127875  -55.19778547 -152.54501861]\n",
      " ...\n",
      " [  -5.97839689   -1.24571797  -71.08900602  -48.02314716 -128.86857384]\n",
      " [  -6.95381452   -1.5069233   -83.69098112  -56.03339324 -155.38579388]\n",
      " [  -8.14998226   -1.42374813  -98.75384348  -66.52032494 -183.01166646]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=False\n",
      "exit seve_checkpoint\n",
      "Epoch 0 Loss: 17703.215391369708\n",
      "enter forward, training=True\n",
      "if self.training is True\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[-2.08600244e+00 -4.05303936e-02 -2.64480785e+01 -1.79828788e+01\n",
      "  -5.04271247e+01]\n",
      " [-1.92295871e+00 -3.42664729e-01 -2.46768047e+01 -1.60750159e+01\n",
      "  -5.01876205e+01]]\n",
      "\tself.hidden_layer_in = [[  -7.47793116   -1.36752662  -90.19118048  -60.79776531 -166.242091  ]\n",
      " [  -7.32170123   -1.47989991  -88.32223323  -59.26106904 -163.86476294]\n",
      " [  -6.85121133   -1.47520375  -82.36127875  -55.19778547 -152.54501861]\n",
      " ...\n",
      " [  -5.97839689   -1.24571797  -71.08900602  -48.02314716 -128.86857384]\n",
      " [  -6.95381452   -1.5069233   -83.69098112  -56.03339324 -155.38579388]\n",
      " [  -8.14998226   -1.42374813  -98.75384348  -66.52032494 -183.01166646]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=True\n",
      "enter backward\n",
      "\tif self.training is True\n",
      "\tafter error_output\n",
      "\tafter back_output\n",
      "\tafter grad_output\n",
      "enter div_act_hidden\n",
      "\tx = [[  -7.47793116   -1.36752662  -90.19118048  -60.79776531 -166.242091  ]\n",
      " [  -7.32170123   -1.47989991  -88.32223323  -59.26106904 -163.86476294]\n",
      " [  -6.85121133   -1.47520375  -82.36127875  -55.19778547 -152.54501861]\n",
      " ...\n",
      " [  -5.97839689   -1.24571797  -71.08900602  -48.02314716 -128.86857384]\n",
      " [  -6.95381452   -1.5069233   -83.69098112  -56.03339324 -155.38579388]\n",
      " [  -8.14998226   -1.42374813  -98.75384348  -66.52032494 -183.01166646]]\n",
      "\texponient = [[5.65425980e-04 2.54736241e-01 6.76811923e-40 3.94335623e-27\n",
      "  6.33836463e-73]\n",
      " [6.61036677e-04 2.27660475e-01 4.38673581e-39 1.83334721e-26\n",
      "  6.83026693e-72]\n",
      " [1.05817312e-03 2.28732117e-01 1.70196692e-36 1.06636617e-24\n",
      "  5.63041199e-67]\n",
      " ...\n",
      " [2.53288353e-03 2.87734250e-01 1.33794100e-31 1.39255445e-21\n",
      "  1.07916908e-56]\n",
      " [9.54985382e-04 2.21590699e-01 4.50265542e-37 4.62387955e-25\n",
      "  3.28705638e-68]\n",
      " [2.88740482e-04 2.40809736e-01 1.29345317e-43 1.29000069e-29\n",
      "  3.30402355e-80]]\n",
      "\texponient_minus = [[1.76857809e+03 3.92562909e+00 1.47751534e+39 2.53591089e+26\n",
      "  1.57769402e+72]\n",
      " [1.51277536e+03 4.39250599e+00 2.27959933e+38 5.45450416e+25\n",
      "  1.46407163e+71]\n",
      " [9.45024951e+02 4.37192648e+00 5.87555486e+35 9.37764185e+23\n",
      "  1.77606897e+66]\n",
      " ...\n",
      " [3.94806942e+02 3.47542915e+00 7.47417114e+30 7.18104773e+20\n",
      "  9.26638857e+55]\n",
      " [1.04713645e+03 4.51282479e+00 2.22091168e+36 2.16268609e+24\n",
      "  3.04223562e+67]\n",
      " [3.46331762e+03 4.15265602e+00 7.73124242e+42 7.75193383e+28\n",
      "  3.02661281e+79]]\n",
      "\ty = [[2.26170320e-03 9.56854169e-01 2.70724769e-39 1.57734249e-26\n",
      "  2.53534585e-72]\n",
      " [2.64414555e-03 8.65769670e-01 1.75469432e-38 7.33338885e-26\n",
      "  2.73210677e-71]\n",
      " [4.23268774e-03 8.69440736e-01 6.80786767e-36 4.26546467e-24\n",
      "  2.25216480e-66]\n",
      " ...\n",
      " [1.01314691e-02 1.06293551e+00 5.35176400e-31 5.57021781e-21\n",
      "  4.31667631e-56]\n",
      " [3.81993804e-03 8.44877264e-01 1.80106217e-36 1.84955182e-24\n",
      "  1.31482255e-67]\n",
      " [1.15496183e-03 9.10442967e-01 5.17381267e-43 5.16000276e-29\n",
      "  1.32160942e-79]]\n",
      "exit div_act_hidden\n",
      "\tafter back_hidden\n",
      "\n",
      "\tafter grad_hidden\n",
      "\n",
      "exit backward\n",
      "enter seve_checkpoint\n",
      "enter forward, training=False\n",
      "if self.training is False\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[ 6.48472884e+02  1.89874750e+05 -2.64480785e+01 -1.79828788e+01\n",
      "  -5.04271247e+01]\n",
      " [ 6.56900463e+02  1.78929387e+05 -2.46768047e+01 -1.60750159e+01\n",
      "  -5.01876205e+01]]\n",
      "\tself.hidden_layer_in = [[ 2.64278025e+03  5.90858296e+05 -9.01911805e+01 -6.07977653e+01\n",
      "  -1.66242091e+02]\n",
      " [ 2.61655876e+03  5.78110260e+05 -8.83222332e+01 -5.92610690e+01\n",
      "  -1.63864763e+02]\n",
      " [ 2.47115888e+03  5.35341453e+05 -8.23612788e+01 -5.51977855e+01\n",
      "  -1.52545019e+02]\n",
      " ...\n",
      " [ 2.15867854e+03  4.53183358e+05 -7.10890060e+01 -4.80231472e+01\n",
      "  -1.28868574e+02]\n",
      " [ 2.50881056e+03  5.45060010e+05 -8.36909811e+01 -5.60333932e+01\n",
      "  -1.55385794e+02]\n",
      " [ 2.86005729e+03  6.52581315e+05 -9.87538435e+01 -6.65203249e+01\n",
      "  -1.83011666e+02]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=False\n",
      "exit seve_checkpoint\n",
      "Epoch 1 Loss: nan\n",
      "enter forward, training=True\n",
      "if self.training is True\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[ 6.48472884e+02  1.89874750e+05 -2.64480785e+01 -1.79828788e+01\n",
      "  -5.04271247e+01]\n",
      " [ 6.56900463e+02  1.78929387e+05 -2.46768047e+01 -1.60750159e+01\n",
      "  -5.01876205e+01]]\n",
      "\tself.hidden_layer_in = [[ 2.64278025e+03  5.90858296e+05 -9.01911805e+01 -6.07977653e+01\n",
      "  -1.66242091e+02]\n",
      " [ 2.61655876e+03  5.78110260e+05 -8.83222332e+01 -5.92610690e+01\n",
      "  -1.63864763e+02]\n",
      " [ 2.47115888e+03  5.35341453e+05 -8.23612788e+01 -5.51977855e+01\n",
      "  -1.52545019e+02]\n",
      " ...\n",
      " [ 2.15867854e+03  4.53183358e+05 -7.10890060e+01 -4.80231472e+01\n",
      "  -1.28868574e+02]\n",
      " [ 2.50881056e+03  5.45060010e+05 -8.36909811e+01 -5.60333932e+01\n",
      "  -1.55385794e+02]\n",
      " [ 2.86005729e+03  6.52581315e+05 -9.87538435e+01 -6.65203249e+01\n",
      "  -1.83011666e+02]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=True\n",
      "enter backward\n",
      "\tif self.training is True\n",
      "\tafter error_output\n",
      "\tafter back_output\n",
      "\tafter grad_output\n",
      "enter div_act_hidden\n",
      "\tx = [[ 2.64278025e+03  5.90858296e+05 -9.01911805e+01 -6.07977653e+01\n",
      "  -1.66242091e+02]\n",
      " [ 2.61655876e+03  5.78110260e+05 -8.83222332e+01 -5.92610690e+01\n",
      "  -1.63864763e+02]\n",
      " [ 2.47115888e+03  5.35341453e+05 -8.23612788e+01 -5.51977855e+01\n",
      "  -1.52545019e+02]\n",
      " ...\n",
      " [ 2.15867854e+03  4.53183358e+05 -7.10890060e+01 -4.80231472e+01\n",
      "  -1.28868574e+02]\n",
      " [ 2.50881056e+03  5.45060010e+05 -8.36909811e+01 -5.60333932e+01\n",
      "  -1.55385794e+02]\n",
      " [ 2.86005729e+03  6.52581315e+05 -9.87538435e+01 -6.65203249e+01\n",
      "  -1.83011666e+02]]\n",
      "\texponient = [[           inf            inf 6.76811923e-40 3.94335623e-27\n",
      "  6.33836463e-73]\n",
      " [           inf            inf 4.38673581e-39 1.83334721e-26\n",
      "  6.83026693e-72]\n",
      " [           inf            inf 1.70196692e-36 1.06636617e-24\n",
      "  5.63041199e-67]\n",
      " ...\n",
      " [           inf            inf 1.33794100e-31 1.39255445e-21\n",
      "  1.07916908e-56]\n",
      " [           inf            inf 4.50265542e-37 4.62387955e-25\n",
      "  3.28705638e-68]\n",
      " [           inf            inf 1.29345317e-43 1.29000069e-29\n",
      "  3.30402355e-80]]\n",
      "\texponient_minus = [[0.00000000e+00 0.00000000e+00 1.47751534e+39 2.53591089e+26\n",
      "  1.57769402e+72]\n",
      " [0.00000000e+00 0.00000000e+00 2.27959933e+38 5.45450416e+25\n",
      "  1.46407163e+71]\n",
      " [0.00000000e+00 0.00000000e+00 5.87555486e+35 9.37764185e+23\n",
      "  1.77606897e+66]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 7.47417114e+30 7.18104773e+20\n",
      "  9.26638857e+55]\n",
      " [0.00000000e+00 0.00000000e+00 2.22091168e+36 2.16268609e+24\n",
      "  3.04223562e+67]\n",
      " [0.00000000e+00 0.00000000e+00 7.73124242e+42 7.75193383e+28\n",
      "  3.02661281e+79]]\n",
      "\ty = [[0.00000000e+00 0.00000000e+00 2.70724769e-39 1.57734249e-26\n",
      "  2.53534585e-72]\n",
      " [0.00000000e+00 0.00000000e+00 1.75469432e-38 7.33338885e-26\n",
      "  2.73210677e-71]\n",
      " [0.00000000e+00 0.00000000e+00 6.80786767e-36 4.26546467e-24\n",
      "  2.25216480e-66]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 5.35176400e-31 5.57021781e-21\n",
      "  4.31667631e-56]\n",
      " [0.00000000e+00 0.00000000e+00 1.80106217e-36 1.84955182e-24\n",
      "  1.31482255e-67]\n",
      " [0.00000000e+00 0.00000000e+00 5.17381267e-43 5.16000276e-29\n",
      "  1.32160942e-79]]\n",
      "exit div_act_hidden\n",
      "\tafter back_hidden\n",
      "\n",
      "\tafter grad_hidden\n",
      "\n",
      "exit backward\n",
      "enter seve_checkpoint\n",
      "enter forward, training=False\n",
      "if self.training is False\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\tself.hidden_layer_in = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=False\n",
      "exit seve_checkpoint\n",
      "Epoch 2 Loss: nan\n",
      "enter forward, training=True\n",
      "if self.training is True\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\tself.hidden_layer_in = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=True\n",
      "enter backward\n",
      "\tif self.training is True\n",
      "\tafter error_output\n",
      "\tafter back_output\n",
      "\tafter grad_output\n",
      "enter div_act_hidden\n",
      "\tx = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\texponient = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\texponient_minus = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\ty = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "exit div_act_hidden\n",
      "\tafter back_hidden\n",
      "\n",
      "\tafter grad_hidden\n",
      "\n",
      "exit backward\n",
      "enter seve_checkpoint\n",
      "enter forward, training=False\n",
      "if self.training is False\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\tself.hidden_layer_in = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=False\n",
      "exit seve_checkpoint\n",
      "Epoch 3 Loss: nan\n",
      "enter forward, training=True\n",
      "if self.training is True\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\tself.hidden_layer_in = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=True\n",
      "enter backward\n",
      "\tif self.training is True\n",
      "\tafter error_output\n",
      "\tafter back_output\n",
      "\tafter grad_output\n",
      "enter div_act_hidden\n",
      "\tx = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\texponient = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\texponient_minus = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\ty = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "exit div_act_hidden\n",
      "\tafter back_hidden\n",
      "\n",
      "\tafter grad_hidden\n",
      "\n",
      "exit backward\n",
      "enter seve_checkpoint\n",
      "enter forward, training=False\n",
      "if self.training is False\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\tself.hidden_layer_in = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=False\n",
      "exit seve_checkpoint\n",
      "Epoch 4 Loss: nan\n",
      "enter forward, training=True\n",
      "if self.training is True\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\tself.hidden_layer_in = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=True\n",
      "enter backward\n",
      "\tif self.training is True\n",
      "\tafter error_output\n",
      "\tafter back_output\n",
      "\tafter grad_output\n",
      "enter div_act_hidden\n",
      "\tx = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\texponient = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\texponient_minus = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\ty = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "exit div_act_hidden\n",
      "\tafter back_hidden\n",
      "\n",
      "\tafter grad_hidden\n",
      "\n",
      "exit backward\n",
      "enter seve_checkpoint\n",
      "enter forward, training=False\n",
      "if self.training is False\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\tself.hidden_layer_in = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=False\n",
      "exit seve_checkpoint\n",
      "Epoch 5 Loss: nan\n",
      "enter forward, training=True\n",
      "if self.training is True\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\tself.hidden_layer_in = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=True\n",
      "enter backward\n",
      "\tif self.training is True\n",
      "\tafter error_output\n",
      "\tafter back_output\n",
      "\tafter grad_output\n",
      "enter div_act_hidden\n",
      "\tx = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\texponient = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\texponient_minus = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\ty = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "exit div_act_hidden\n",
      "\tafter back_hidden\n",
      "\n",
      "\tafter grad_hidden\n",
      "\n",
      "exit backward\n",
      "enter seve_checkpoint\n",
      "enter forward, training=False\n",
      "if self.training is False\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\tself.hidden_layer_in = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=False\n",
      "exit seve_checkpoint\n",
      "Epoch 6 Loss: nan\n",
      "enter forward, training=True\n",
      "if self.training is True\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\tself.hidden_layer_in = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=True\n",
      "enter backward\n",
      "\tif self.training is True\n",
      "\tafter error_output\n",
      "\tafter back_output\n",
      "\tafter grad_output\n",
      "enter div_act_hidden\n",
      "\tx = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\texponient = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\texponient_minus = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\ty = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "exit div_act_hidden\n",
      "\tafter back_hidden\n",
      "\n",
      "\tafter grad_hidden\n",
      "\n",
      "exit backward\n",
      "enter seve_checkpoint\n",
      "enter forward, training=False\n",
      "if self.training is False\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\tself.hidden_layer_in = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=False\n",
      "exit seve_checkpoint\n",
      "Epoch 7 Loss: nan\n",
      "enter forward, training=True\n",
      "if self.training is True\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\tself.hidden_layer_in = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=True\n",
      "enter backward\n",
      "\tif self.training is True\n",
      "\tafter error_output\n",
      "\tafter back_output\n",
      "\tafter grad_output\n",
      "enter div_act_hidden\n",
      "\tx = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\texponient = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\texponient_minus = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\ty = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "exit div_act_hidden\n",
      "\tafter back_hidden\n",
      "\n",
      "\tafter grad_hidden\n",
      "\n",
      "exit backward\n",
      "enter seve_checkpoint\n",
      "enter forward, training=False\n",
      "if self.training is False\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\tself.hidden_layer_in = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=False\n",
      "exit seve_checkpoint\n",
      "Epoch 8 Loss: nan\n",
      "enter forward, training=True\n",
      "if self.training is True\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\tself.hidden_layer_in = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=True\n",
      "enter backward\n",
      "\tif self.training is True\n",
      "\tafter error_output\n",
      "\tafter back_output\n",
      "\tafter grad_output\n",
      "enter div_act_hidden\n",
      "\tx = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\texponient = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\texponient_minus = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\ty = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "exit div_act_hidden\n",
      "\tafter back_hidden\n",
      "\n",
      "\tafter grad_hidden\n",
      "\n",
      "exit backward\n",
      "enter seve_checkpoint\n",
      "enter forward, training=False\n",
      "if self.training is False\n",
      "\n",
      "X = [[0.76914527 0.35667445]\n",
      " [0.34578193 0.73468934]\n",
      " [0.10681338 0.74924975]\n",
      " ...\n",
      " [0.33011004 0.05312882]\n",
      " [0.0662443  0.84661554]\n",
      " [0.960987   0.49805491]]\n",
      "self.weights_input_hidden = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "\tself.hidden_layer_in = [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "enter act_hidden\n",
      "exit act_hidden\n",
      "exit forward, training=False\n",
      "exit seve_checkpoint\n",
      "Epoch 9 Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vicky\\AppData\\Local\\Temp\\ipykernel_22260\\2137258666.py:60: RuntimeWarning: overflow encountered in power\n",
      "  exponient = np.e**(2*self.param_hidden*x)\n",
      "C:\\Users\\vicky\\AppData\\Local\\Temp\\ipykernel_22260\\2137258666.py:62: RuntimeWarning: invalid value encountered in divide\n",
      "  y = (exponient-1)/(exponient+1)\n",
      "C:\\Users\\vicky\\AppData\\Local\\Temp\\ipykernel_22260\\2137258666.py:72: RuntimeWarning: overflow encountered in power\n",
      "  exponient = np.e**(self.param_hidden*x)\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "nn_1 = shallow_nn(5, 1, 1, 0.01)\n",
    "ttl_loss = []\n",
    "ttl_param = []\n",
    "\n",
    "loss, param = get_checkpoint(nn_1)\n",
    "ttl_loss.append(loss)\n",
    "ttl_param.append(param)\n",
    "\n",
    "for epoch in range(10):\n",
    "    # Forward pass\n",
    "    nn_1.forward(X_test_n, training=True)\n",
    "    \n",
    "    # BackWard pass\n",
    "    nn_1.backward(Y_test_n)\n",
    "\n",
    "    # compute the loss\n",
    "    loss, param = get_checkpoint(nn_1)\n",
    "    ttl_loss.append(loss)\n",
    "    ttl_param.append(param)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch} Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __init__(self, n, param_hidden, param_output, learning_rate)\n",
    "nn = shallow_nn(2,1,1,0.01)\n",
    "# nn.evaluate(X_test_n,Y_test_n)\n",
    "print(f\"type(X_test_n) = {type(X_test_n)}\")\n",
    "print()\n",
    "pred = nn.predict(X_test_n)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703,\n",
       " 1.6503305007690703]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
